{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import numpy as onp\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "import scipy\n",
    "from neuralss import ss_init, ss_apply\n",
    "import nonlinear_benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.key(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_default_device\", jax.devices(\"cpu\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = Path(\"out\") / f\"maml_10s.p\"\n",
    "ckpt = pickle.load(open(ckpt_path, \"rb\"))\n",
    "\n",
    "cfg = ckpt[\"cfg\"]\n",
    "params_maml = ckpt[\"params\"]\n",
    "scalers = ckpt[\"scalers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_lens = [100, 200, 400, 500]#, 600, 800, 1_000, 2000, 3000, 4000, 5000]\n",
    "train_lens = [100, 200, 400, 500, 600, 800, 1_000, 2000, 3000, 4000, 5000]\n",
    "mc_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"bwdataset\"\n",
    "data_folder = Path(data_folder)\n",
    "data = scipy.io.loadmat(data_folder / \"bw_matlab.mat\")\n",
    "\n",
    "y_train = data[\"y\"] / 7e-4\n",
    "u_train = data[\"u\"] / 50.0\n",
    "\n",
    "y_test = scipy.io.loadmat(data_folder / \"yval_multisine.mat\")[\"yval_multisine\"].reshape(-1, 1) / 7e-4\n",
    "u_test = scipy.io.loadmat(data_folder / \"uval_multisine.mat\")[\"uval_multisine\"].reshape(-1, 1) / 50.0\n",
    "N = y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error loss function\n",
    "def mse_loss_x0_fn(ov, y, u):\n",
    "    y_hat = ss_apply(ov[\"params\"], scalers, ov[\"x0\"], u)\n",
    "    err = y - y_hat\n",
    "    loss = jnp.mean(err ** 2)\n",
    "    return loss\n",
    "\n",
    "def train(ov, y, u, iters=10, lr=0.1):\n",
    "\n",
    "        loss_cfg = partial(mse_loss_x0_fn, y=y, u=u)\n",
    "#        opt = optax.adamw(learning_rate=lr)\n",
    "        opt = optax.sgd(learning_rate=lr)\n",
    "        state = train_state.TrainState.create(apply_fn=loss_cfg, params=ov, tx=opt)\n",
    "\n",
    "        @jax.jit\n",
    "        def make_step(state):\n",
    "                loss, grads = jax.value_and_grad(state.apply_fn)(state.params)\n",
    "                state = state.apply_gradients(grads=grads)\n",
    "                return loss, state\n",
    "        \n",
    "        losses = jnp.empty(iters)\n",
    "        for idx in (pbar := tqdm(range(iters))):\n",
    "                loss, state = make_step(state)\n",
    "                losses = losses.at[idx].set(loss)\n",
    "                #if idx % 100 == 0:\n",
    "                #    pbar.set_postfix_str(loss.item())\n",
    "\n",
    "        return state.params, jnp.array(losses)\n",
    "\n",
    "\n",
    "def train_adamw(ov, y, u, iters=10_000, lr=1e-3):\n",
    "\n",
    "        loss_cfg = partial(mse_loss_x0_fn, y=y, u=u)\n",
    "        opt = optax.adamw(learning_rate=lr)\n",
    "#        opt = optax.sgd(learning_rate=lr)\n",
    "        state = train_state.TrainState.create(apply_fn=loss_cfg, params=ov, tx=opt)\n",
    "\n",
    "        @jax.jit\n",
    "        def make_step(state):\n",
    "                loss, grads = jax.value_and_grad(state.apply_fn)(state.params)\n",
    "                state = state.apply_gradients(grads=grads)\n",
    "                return loss, state\n",
    "        \n",
    "        losses = jnp.empty(iters)\n",
    "        for idx in (pbar := tqdm(range(iters))):\n",
    "                loss, state = make_step(state)\n",
    "                losses = losses.at[idx].set(loss)\n",
    "                #if idx % 100 == 0:\n",
    "                #pbar.set_postfix_str(loss.item())\n",
    "\n",
    "        return state.params, jnp.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = onp.empty((len(train_lens), mc_size))\n",
    "fit_tr = onp.empty((len(train_lens), mc_size))\n",
    "train_time = onp.empty(len(train_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train mc_size models in parallel!\n",
    "\n",
    "for len_idx, train_len in enumerate(train_lens):\n",
    "\n",
    "    print(f\"Processing length {train_len}...\")\n",
    "    \n",
    "    # generate mc sequences\n",
    "    key, subkey = jr.split(key)     \n",
    "    start_indexes = jr.randint(subkey, shape=(mc_size,),  minval=0, maxval=N-train_len)\n",
    "    mc_indexes = start_indexes[:, None] + jnp.arange(train_len)\n",
    "    mc_y, mc_u = y_train[mc_indexes], u_train[mc_indexes]\n",
    "\n",
    "    # train models\n",
    "    time_start = time.time()\n",
    "    print(f\"Training  {mc_size} full models starting from MAML initialization with SGD...\")\n",
    "    key, subkey = jr.split(key)\n",
    "    keys_init = jr.split(subkey, mc_size)\n",
    "    opt_vars_init = {\"params\": params_maml, \"x0\": jnp.zeros((cfg.nx,))}\n",
    "    opt_vars_adam, losses_full = jax.vmap(train_adamw, in_axes=(None, 0, 0))(opt_vars_init, mc_y, mc_u)\n",
    "    train_time[len_idx] = time.time() - time_start\n",
    "\n",
    "    # test adam models\n",
    "    x0 = jnp.zeros((cfg.nx, ))\n",
    "    y_test_hat = jax.vmap(ss_apply, in_axes=(0, None, None, None))(opt_vars_adam[\"params\"], scalers, x0, u_test)\n",
    "    y_train_hat = jax.vmap(ss_apply, in_axes=(0, None, 0, 0))(opt_vars_adam[\"params\"], scalers, opt_vars_adam[\"x0\"], mc_u)\n",
    "    for mc_idx in range(mc_size):\n",
    "        fit[len_idx, mc_idx] = nonlinear_benchmarks.error_metrics.fit_index(y_test[cfg.skip_loss:], y_test_hat[mc_idx, cfg.skip_loss:])[0]\n",
    "        fit_tr[len_idx, mc_idx] = nonlinear_benchmarks.error_metrics.fit_index(mc_y[mc_idx, :], y_train_hat[mc_idx, :])[0]\n",
    "\n",
    "\n",
    "    train_time[len_idx] = time.time() - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final checkpoint\n",
    "ckpt = {\n",
    "    \"train_lens\": train_lens,\n",
    "    \"train_time\": train_time,\n",
    "    \"fit\": fit,\n",
    "    \"fit_tr\": fit_tr,\n",
    "}\n",
    "\n",
    "ckpt_path = Path(\"out\") / f\"mc_maml.p\"\n",
    "ckpt_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "pickle.dump(ckpt, open(ckpt_path, \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
