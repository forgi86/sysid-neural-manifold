{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import set_size, tex_fonts, LINEWIDTH_L_CSS as linewidth\n",
    "import pickle\n",
    "import scipy\n",
    "from pathlib import Path\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import numpy as onp\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "from neuralss import ss_init, ss_apply\n",
    "import nonlinear_benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nin = 10 # state_initialization_window_length variable in the NonlinearBenchmarks jargon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "cfg = {\n",
    "    \"nu\": 1,\n",
    "    \"ny\": 1,\n",
    "    \"nx\": 3,\n",
    "    \"hidden_f\": 16,\n",
    "    \"hidden_g\": 16,\n",
    "    \"skip_loss\": 500,\n",
    "}\n",
    "cfg = Namespace(**cfg)\n",
    "\n",
    "scalers = {\"f\": {\"lin\": 1e-2, \"nl\": 1e-2}, \"g\": {\"lin\": 1e0, \"nl\": 1e0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams.update(tex_fonts) # use latex fonts\n",
    "plt.rcParams.update({\"axes.grid\": True}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "jax.config.update(\"jax_default_device\", jax.devices(\"cpu\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(\"out\") / \"full_alldata.pkl\" \n",
    "ckpt = pickle.load(open(filename, \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"bwdataset\"\n",
    "data_folder = Path(data_folder)\n",
    "\n",
    "u_test = scipy.io.loadmat(data_folder / \"uval_multisine.mat\")[\"uval_multisine\"].reshape(-1, 1)\n",
    "y_test = scipy.io.loadmat(data_folder / \"yval_multisine.mat\")[\"yval_multisine\"].reshape(-1, 1)\n",
    "\n",
    "u_test = u_test / 50.0\n",
    "y_test = y_test / 7e-4\n",
    "t_test = onp.arange(0, len(u_test)) / 750 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the initial system state x0 in the first samples of the test set\n",
    "\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "from tqdm import tqdm\n",
    "\n",
    "#x0_opt = jnp.ones((cfg.nx, ))\n",
    "x0_opt = ckpt[\"x0\"]\n",
    "\n",
    "def loss_state(x0):\n",
    "    y_hat = ss_apply(ckpt[\"params\"], scalers, x0, u_test[:nin, :])\n",
    "    loss = jnp.mean((y_test[:nin] - y_hat)**2)\n",
    "    return loss\n",
    "\n",
    "def train_x0(x0_opt, iters=100, lr=1e-2):\n",
    "\n",
    "        opt = optax.adamw(learning_rate=lr)\n",
    "        state = train_state.TrainState.create(apply_fn=loss_state, params=x0_opt, tx=opt)\n",
    "\n",
    "        @jax.jit\n",
    "        def make_step(state):\n",
    "                loss, grads = jax.value_and_grad(state.apply_fn)(state.params)\n",
    "                state = state.apply_gradients(grads=grads)\n",
    "                return loss, state\n",
    "        \n",
    "        losses = jnp.empty(iters)\n",
    "        for idx in (pbar := tqdm(range(iters))):\n",
    "                loss, state = make_step(state)\n",
    "                losses = losses.at[idx].set(loss)\n",
    "                pbar.set_postfix_str(loss.item())\n",
    "\n",
    "        return state.params, jnp.array(losses)\n",
    "\n",
    "x0_opt, _ = train_x0(x0_opt, iters=1000, lr=1e-2)\n",
    "#loss_state(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = x0_opt\n",
    "y_hat = ss_apply(ckpt[\"params\"], scalers, x0, u_test)\n",
    "#y2_hat = ss_apply(opt_vars_adam[\"params\"], scalers, x0, u2)\n",
    "plt.figure()\n",
    "plt.plot(t_test, y_test, \"k\", label=\"true\")\n",
    "plt.plot(t_test, y_hat, \"b\", label=\"reconstructed\")\n",
    "plt.plot(t_test, y_test - y_hat, \"r\", label=\"reconstruction error\")\n",
    "plt.xlabel(\"time (s)\")\n",
    "#plt.axvline(t_test[cfg.skip_loss], color=\"k\") # to instead skip first samples from the loss\n",
    "plt.ylim([-4, 4])\n",
    "#plt.xlim([5, 6])\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(Path(\"fig\") / \"full_order_prediction.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_full = nonlinear_benchmarks.error_metrics.fit_index(y_test[cfg.skip_loss:], y_hat[cfg.skip_loss:])\n",
    "rmse_full = nonlinear_benchmarks.error_metrics.RMSE(y_test[cfg.skip_loss:], y_hat[cfg.skip_loss:])*7e-4 * 1e6\n",
    "\n",
    "#fit_full = nonlinear_benchmarks.error_metrics.fit_index(y_test, y_hat)\n",
    "#rmse_full = nonlinear_benchmarks.error_metrics.RMSE(y_test, y_hat)*7e-4 * 1e6\n",
    "fit_full, rmse_full # (Array([98.54584449], dtype=float64), array([0.96720211]))\n",
    "print(f\"Fit index: {fit_full[0]:.2f} %\")\n",
    "print(f\"RMSE: {rmse_full[0]:.2f}e-6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w, v = onp.linalg.eigh(onp.array(data[\"H\"]))#[:-3, :-3])\n",
    "w, v = jnp.linalg.eigh(ckpt[\"H\"][:-3, :-3])#[:-3, :-3])\n",
    "w, v = jnp.linalg.eigh(ckpt[\"H\"])\n",
    "w = w[::-1] # eigvals\n",
    "v = v[:, ::-1] # eigvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=set_size(linewidth, fraction=1.0))\n",
    "#plt.title(\"Hessian eigenvalues\")\n",
    "plt.plot(w[1:], \"k*\")\n",
    "plt.xlabel(\"Eigenvalue index\")\n",
    "plt.ylabel(\"Eigenvalue\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(\"fig\") / \"hessian_eigenvalues.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, figsize=set_size(linewidth, fraction=1.0))\n",
    "# #plt.title(\"Hessian eigenvalues\")\n",
    "# plt.semilogy(w, \"k*\")\n",
    "# plt.xlabel(\"Eigenvalue index\")\n",
    "# plt.ylabel(\"Eigenvalue\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(Path(\"fig\") / \"hessian_eigenvalues.pdf\")\n",
    "# #plt.ylim([1e-5, 1e3]);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
