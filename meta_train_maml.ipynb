{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "from dataset.input.signals import multisine_signal\n",
    "import dataset.dynamics.boucwen as dyn\n",
    "from dataset.simulate import simulate_rk4 as simulate\n",
    "from dataset.simulate import generate_batch\n",
    "from neuralss import ss_init, ss_apply\n",
    "from ae import Encoder, Projector\n",
    "from lr import create_learning_rate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_default_device\", jax.devices(\"gpu\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "cfg = {\n",
    "    # Misc\n",
    "    \"log_wandb\": True,\n",
    "\n",
    "    # Meta dataset\n",
    "    \"K\": 2,  # repetitions from the same system, unused\n",
    "    \"nu\": 1,\n",
    "    \"ny\": 1,\n",
    "    \"seq_len\": 1500,\n",
    "    \"skip_sim\": 500,\n",
    "    \"fs\": 750.0, # sampling time\n",
    "    \"fh\": 150, # highest frequency\n",
    "    \"upsamp\": 20, # upsampling for integration\n",
    "    \"input_scale\": 50,\n",
    "    \"output_scale\": 7e-4,\n",
    "\n",
    "    # Base learner\n",
    "    \"nx\": 3,\n",
    "    \"hidden_f\": 16,\n",
    "    \"hidden_g\": 16,\n",
    "    \n",
    "    # Inner Loop Optimization\n",
    "    \"alpha\": 0.1,\n",
    "    \"inner_iters\": 10,\n",
    "    \n",
    "    # Optimization\n",
    "    \"batch_size\": 128,  # systems sampled at each meta optimization step\n",
    "    \"iters\": 200_000,\n",
    "    \"lr\": 2e-4,\n",
    "    \"clip\": 1.0,\n",
    "    \"warmup_iters\": 0,\n",
    "    \"skip_loss\": 500,  # skipped from the loss computation, to avoid a more advanced handling of the initial condition\n",
    "    \"same_sys\": 10\n",
    "}\n",
    "\n",
    "cfg = Namespace(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.log_wandb:\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        project=\"sysid-parametric-meta\",\n",
    "        #name=\"run1\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config=vars(cfg)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "key = jr.key(seed)\n",
    "dec_key, proj_key, data_key, train_key = jr.split(key, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta dataset definition\n",
    "fs_up = cfg.fs * cfg.upsamp\n",
    "ts_up = 1.0 / fs_up\n",
    "N = cfg.seq_len + cfg.skip_sim\n",
    "N_up = N * cfg.upsamp\n",
    "\n",
    "input_fn = partial(multisine_signal, seq_len=N_up, fs=fs_up, fh=cfg.fh, scale=cfg.input_scale)\n",
    "simulate_fn = jax.jit(partial(simulate, f_xu=dyn.f_xu))\n",
    "generate_batch = partial(\n",
    "    generate_batch,\n",
    "    init_fn=dyn.init_fn,  # random initial state\n",
    "    input_fn=input_fn,  # random input\n",
    "    params_fn=dyn.params_fn,  # random system parameters\n",
    "    simulate_fn=simulate_fn,  # simulation function\n",
    ")\n",
    "\n",
    "\n",
    "def generate_batches(key, batch_size=cfg.batch_size, K=cfg.K):\n",
    "    generate_batch_cfg = jax.jit(partial(generate_batch, systems=batch_size, runs=K))\n",
    "    while True:\n",
    "        key, subkey = jr.split(key, 2)\n",
    "        yield generate_batch_cfg(subkey)\n",
    "\n",
    "\n",
    "def preproc_batch(batch):\n",
    "    batch_u, batch_x, batch_t, batch_params = batch\n",
    "    batch_y = batch_x[..., [0]]\n",
    "\n",
    "    batch_u /= cfg.input_scale\n",
    "    batch_y /= cfg.output_scale\n",
    "\n",
    "    if cfg.upsamp > 1:\n",
    "        batch_u = scipy.signal.decimate(batch_u, q=cfg.upsamp, axis=-2)\n",
    "        batch_y = scipy.signal.decimate(batch_y, q=cfg.upsamp, axis=-2)\n",
    "\n",
    "    batch_y1 = batch_y[:, 0, cfg.skip_sim:]\n",
    "    batch_u1 = batch_u[:, 0, cfg.skip_sim:]\n",
    "\n",
    "    batch_y2 = batch_y[:, 1, cfg.skip_sim:]\n",
    "    batch_u2 = batch_u[:, 1, cfg.skip_sim:]\n",
    "\n",
    "    return batch_y1, batch_u1, batch_y2, batch_u2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "train_dl = generate_batches(data_key)\n",
    "batch = next(iter(train_dl))\n",
    "batch_y1, batch_u1, batch_y2, batch_u2 = preproc_batch(batch)\n",
    "batch_y1.shape, batch_u1.shape,batch_y2.shape, batch_u2.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize state-space model\n",
    "params_ss = ss_init(dec_key, nu=cfg.nu, ny=cfg.ny, nx=cfg.nx)\n",
    "params_ss_flat, unflatten_dec = jax.flatten_util.ravel_pytree(params_ss)\n",
    "n_params = params_ss_flat.shape[0]\n",
    "scalers = {\"f\": {\"lin\": 1e-2, \"nl\": 1e-2}, \"g\": {\"lin\": 1e0, \"nl\": 1e0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error loss function\n",
    "def mse_loss_fn(p, y, u):\n",
    "    x0 = jnp.zeros((cfg.nx, ))\n",
    "    y1_hat = ss_apply(p, scalers, x0, u)\n",
    "    err = y - y1_hat\n",
    "    loss = jnp.mean(err[cfg.skip_loss:] ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner update function (GD) on a single instance\n",
    "def inner_update_fn(p, y, u, alpha=0.1, iters=1):\n",
    "    grad_fn = jax.grad(mse_loss_fn)\n",
    "    for _ in range(iters):\n",
    "        grads = grad_fn(p, y, u)\n",
    "        inner_sgd_fn = lambda g, p: (p - alpha*g)\n",
    "        p = jax.tree_util.tree_map(inner_sgd_fn, grads, p)\n",
    "    return p\n",
    "\n",
    "# Meta loss (MAML) for one instance\n",
    "def instance_loss_fn(p1, y1, u1, y2, u2):\n",
    "\n",
    "    p2 = inner_update_fn(p1, y1, u1, alpha=cfg.alpha, iters=cfg.inner_iters)\n",
    "    return mse_loss_fn(p2, y2, u2)\n",
    "\n",
    "\n",
    "instance_loss_fn(params_ss, batch_y1[0], batch_u1[0], batch_y2[0], batch_u2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched loss\n",
    "def loss_fn(*args):\n",
    "    loss = jax.vmap(instance_loss_fn, in_axes=(None, 0, 0, 0, 0))(*args)\n",
    "    return jnp.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_scheduler = create_learning_rate_fn(cfg)\n",
    "\n",
    "opt = optax.chain(\n",
    "  optax.clip(cfg.clip),\n",
    "  optax.adam(learning_rate=cfg.lr),\n",
    ")\n",
    "state = train_state.TrainState.create(apply_fn=loss_fn, params=params_ss, tx=opt)\n",
    "\n",
    "@jax.jit\n",
    "def make_step(state, y1, u1, y2, u2):\n",
    "        loss, grads = jax.value_and_grad(state.apply_fn)(state.params, y1, u1, y2, u2)\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        return loss, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS = []\n",
    "loss = jnp.array(jnp.nan)\n",
    "#for itr, batch in (pbar := tqdm(enumerate(train_dl), total=cfg.iters)):\n",
    "\n",
    "for itr in (pbar := tqdm(range(cfg.iters))):\n",
    "\n",
    "    if itr % cfg.same_sys == 0: # some speed up\n",
    "        batch = next(iter(train_dl))\n",
    "        batch_y1, batch_u1, batch_y2, batch_u2 = preproc_batch(batch)\n",
    "\n",
    "    loss, new_state = make_step(state, batch_y1, batch_u1, batch_y2, batch_u2)\n",
    "    if not jnp.isnan(loss).any() and loss < 2.0: # fix some instability issues in training\n",
    "        state = new_state\n",
    "\n",
    "    LOSS.append(loss.item())\n",
    "    if itr % 10 == 0:\n",
    "        pbar.set_postfix_str(\n",
    "            f\"loss:{loss.item():.4f}\"\n",
    "        )\n",
    "\n",
    "    #if itr % 100 == 0 and cfg.log_wandb:\n",
    "    if cfg.log_wandb:\n",
    "        if itr % 1 == 0:\n",
    "            wandb.log({\"loss\": loss.item()})\n",
    "\n",
    "    if itr % 5000 == 0:\n",
    "        ckpt = {\n",
    "            \"cfg\": cfg,\n",
    "            \"params\": state.params,\n",
    "            \"scalers\": scalers,\n",
    "            \"LOSS\": jnp.array(LOSS),\n",
    "        }\n",
    "        ckpt_path = Path(\"tmp\") / f\"maml_{itr}.p\"\n",
    "        ckpt_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "        pickle.dump(ckpt, open(ckpt_path, \"wb\"))\n",
    "\n",
    "\n",
    "    if itr == cfg.iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final checkpoint\n",
    "ckpt = {\n",
    "    \"cfg\": cfg,\n",
    "    \"params\": state.params,\n",
    "    \"scalers\": scalers,\n",
    "    \"LOSS\": jnp.array(LOSS),\n",
    "}\n",
    "\n",
    "ckpt_path = Path(\"out\") / f\"maml.p\"\n",
    "ckpt_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "pickle.dump(ckpt, open(ckpt_path, \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.log_wandb:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
