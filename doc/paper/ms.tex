\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}
\usepackage{cite}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
%\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{booktabs}
\usepackage{authblk}

\newcommand{\base}{{base architecture}}
\newcommand{\ii}{{(i)}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\te}{\mathrm{te}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\seq}[1]{{\mathbf{#1}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\D}{D}
\newcommand{\E}{\mathbb{E}} % expectation
\newcommand{\nsamp}{N}
\newcommand{\sys}{S}
\newcommand{\npar}{n}
\newcommand{\nsvd}{r}
\newcommand{\latent}{z}
\newcommand{\fullpar}{\theta}
\newcommand{\Fullpar}{\Theta}
\newcommand{\redpar}{\phi}
\newcommand{\projpar}{\gamma}
\newcommand{\encpar}{\psi}
\newcommand{\Redpar}{\Phi}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\loss}{\mathcal{L}}
\newtheorem{remark}{Remark}%[section]

\newcommand{\DD}{\mathcal{D}}
\newcommand{\N}{\mathbb{N}}
\newtheorem{problem}{Problem}%[section]

\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}


\title{Manifold meta-learning for reduced-complexity\\neural system identification}

%\author{Marco Forgione, Ankush Chakrabarty, Dario Piga, Matteo Rufolo, Alberto Bemporad}
\author[1]{Marco Forgione}
\author[2]{Ankush Chakrabarty}
\author[1]{Dario Piga}
\author[1]{Matteo Rufolo}
\author[3]{Alberto Bemporad}
%
\affil[1]{SUPSI, IDSIA - Dalle Molle Institute for Artificial Intelligence, Lugano, Switzerland.}
\affil[2]{Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA, USA.}
\affil[3]{IMT School for Advanced Studies Lucca, Lucca, Italy.} 


\begin{document}

%\begin{frontmatter}


\maketitle


\begin{abstract}
System identification has greatly benefited from deep learning techniques, particularly for modeling complex, nonlinear dynamical systems with partially unknown physics where traditional approaches may not be feasible. However, deep learning models often require large datasets and significant computational resources at training and inference due to their high-dimensional parameterizations.
To address this challenge, we propose a meta-learning framework that discovers a low-dimensional manifold within the parameter space of an over-parameterized neural network architecture. This manifold is learned from a meta-dataset of input-output sequences generated by a class of related dynamical systems, enabling efficient model training while preserving the network's expressive power for the considered system class. Unlike bilevel meta-learning approaches, our method employs an auxiliary neural network to map datasets directly onto the learned manifold, eliminating the need for costly second-order gradient computations during meta-training and reducing the number of first-order updates required in inference, which could be expensive for large models. We validate our approach on a family of Bouc-Wen oscillators, which is a well-studied nonlinear system identification benchmark. We demonstrate that we are able to learn accurate models  even in small-data scenarios.
\end{abstract}

\section{Introduction}
In recent years, system identification has advanced significantly with the adoption of deep learning tools and techniques~\cite{forgione2021continuous, Bem25, deshpande2023physics, champneys2024baseline}. These methods have proven particularly effective in modeling complex, nonlinear dynamical systems where the underlying physics is partially or entirely unknown, relying on flexible and expressive black-box components such as neural networks.

However, a fundamental challenge of black-box system identification is the need for large datasets to ensure model reliability across a wide range of operating conditions. Even when such datasets are available, training deep models with numerous parameters can be computationally demanding, limiting their practical applicability. This contrasts with physics-based modeling, which, when feasible, enables data-efficient learning and typically results in models that generalize well beyond the training regime.

To bridge the gap between black-box and physics-based approaches, recent research has explored the opportunity to include domain knowledge into deep-learning architectures and training algorithms, while preserving their flexibility. These efforts include designing custom model structures that satisfy known physical constraints by design~\cite{moradi2023physics, deshpande2023physics, forgione2021continuous} and introducing regularization terms in the loss function to promote the satisfaction of known dynamics, as seen in physics-informed machine learning~\cite{raissi2019physics, nghiem2023physics}. While highly successful, these approaches still require expert knowledge and manual tuning for the design of appropriate architectures and regularizers.

This paper addresses the challenge of learning simplified model architectures in an automated manner using meta-learning techniques~\cite{hospedales2021meta}. In line with the meta-learning setting for system identification~\cite{forgione2023system,chakrabarty2023meta},
we assume access to a \emph{meta-dataset}, namely, a collection of datasets generated by different, yet related, dynamical systems representative of a target population that we aim to describe. The meta-dataset may originate either from a high-fidelity simulator with physical parameters randomized within plausible ranges or from historical real-world data collected in prior experimental campaigns or through monitoring of processes deployed in series.
Additionally, we assume that a given black-box architecture, referred to as the \emph{\base}, is sufficiently expressive to capture the dynamics of each system in the meta-dataset for at least one (system-dependent) choice of its tunable parameters. However, this \base\ is highly over-parameterized, meaning it has significantly more degrees of freedom than the actual factors governing variability across systems in the meta-dataset. 
Given the meta-dataset and the \base,\ our goal is to meta-learn a \emph{low-dimensional manifold} embedded in the \base's parameter space such that the \base,\ with parameters restricted to this manifold, can still describe all the systems within the class of interest. 

Conceptually, our approach is inspired by and builds upon classic approaches for {model-based} meta-learning, which aim at \emph{optimizing the algorithm} (or parts thereof) that estimates a model within a given \base~\cite{hospedales2021meta}. For instance, the celebrated model-agnostic meta-learning (MAML)~\cite{finn2017model} seeks an \emph{initialization} of the \base's\ parameters such that, with a few steps of gradient descent starting from that point, effective adaptation to each dataset in the meta-dataset is achieved. This meta-learning problem is tackled through bilevel optimization, where the initial parameters are optimized at the outer level, while adaptation to the specific datasets is performed via gradient descent at the inner level.
Unlike MAML, our methodology meta-learns a low-dimensional manifold in the \base's\ parameter space instead of providing an initialization for optimization, and can be related to learning an effective {\it inductive bias}~\cite{goyal2022inductive} to facilitate the system identification procedure, in terms of both fitting the model parameters and required training data.

An existing meta-learning algorithm that even more closely resembles our approach is latent embedding optimization (LEO)~\cite{rusu2018meta} which, similarly to our approach, meta-learns a low-dimensional embedding of the datasets. Compared to LEO (and MAML), we circumvent the need to perform bilevel optimization by introducing an \emph{auxiliary neural network}, estimated together with the manifold, that directly maps each dataset onto its low-dimensional representation, bypassing the need for gradient-based inner-loop optimization. 
Furthermore, we formalize our methodology in a regression setting rather than a classification one and demonstrate its application to system identification using the Bouc-Wen benchmark~\cite{noel2016hysteretic}.

Meta-learning has been recently applied for system identification, among other works, in~\cite{du2023can,forgione2023system,chakrabarty2025meta}. However, \cite{du2023can,forgione2023system} build a unified \emph{meta-model} for the entire meta-dataset, relying on the \emph{in-context} learning capabilities of transformer architectures. Conversely,~\cite{chakrabarty2025meta} is based on the MAML algorithm and its variants; unlike the current approach, MAML-like approaches involve a bilevel training procedure. While this is effective, a major drawback is the complexity of the bilevel training procedure. To avoid this, workarounds such as implicit MAML (iMAML) has been investigated by~\cite{yan2024mpc}, which leverages the implicit function theorem to simplify the inner-loop training. Also,~\cite{chakrabarty2024physics} demonstrates the potential of almost-no-inner-loop (ANIL), which restricts the inner-loop training to a small set of layers or components in the network. 
In contrast to ANIL, our method enables the meta-learning procedure to discover a low-dimensional manifold in parameter space where adaptation occurs, rather than specifying in advance which components of the \base\ should be adapted.

The rest of the paper is organized as follows. The meta-learning framework is introduced in Section~\ref{sec:framework} and the meta-learning procedure is detailed in Section~\ref{sec:reduced_order}. Numerical results are presented in Section~\ref{sec:examples}, and conclusions are drawn in Section~\ref{sec:conclusions}.

\section{Framework}
\label{sec:framework}
\subsection{Data stream and distribution}
In line with the meta-modeling framework introduced in~\cite{forgione2023system}, we assume to have access to a (possibly unlimited) collection $\DD$ of datasets referred to as the meta-dataset, described by 
\begin{equation}  
\label{eq:dataset_stream}
\DD=\{D^\ii\}_{i\in\N}, \text{ where } D^\ii = (\seq u_{0:N-1}^\ii, \seq y_{0:N-1}^\ii).%,\,\, i=0,1,\dots.
\end{equation}
Each dataset $D^\ii$ comprises $N$
inputs $\seq u^\ii_k \in \R^{n_u}$
and corresponding targets $\seq y^\ii_k \in \R^{n_y}, k=0,1,\dots,N$.
In the context of system identification, $\seq y^\ii_{0:N-1}$ is the sequence of outputs generated by a dynamical system $S^\ii$ driven by the sequence of control actions $\seq u^\ii_{0:N-1}$. This results in a causal  dependency $\seq u_{0:k}^\ii \rightarrow \seq y_k^\ii$. In the case of static regression, the dependency is simply $\seq u_k^\ii \rightarrow \seq y_k^\ii$.

In the following, we adopt the shorthand notation $\seq u=\seq u_{0:N-1}, \seq y = \seq y_{0:N-1}$ to denote the sequence of $N$ input and output samples, respectively, contained in a dataset.
The inputs $\seq u^\ii$ of each dataset are assumed to be drawn from a distribution with density $p(\seq u)$, while the dependency 
$\seq u^\ii \rightarrow \seq y^\ii$
is different, yet related across datasets.
Formally, the collection of datasets~\eqref{eq:dataset_stream} may be interpreted as samples drawn from a distribution (in the space of datasets) with density function:
\begin{equation}
\label{eq:dataset_distribution}
p(D) = p(\seq{u}, \seq{y}) = p(\seq{u}) 
%\overbrace{
\int_\latent p(\latent)  
p(\seq{y} | \seq{u}, \latent) \; d\latent,
%}
%^{p(\seq y | \seq u)}
\end{equation}
where $\latent \in \R^{n_\latent}$ is some unknown or hidden variable that characterizes the data-generating mechanism $p(\seq{y} | \seq{u}, \latent)$.

In a synthetic data generation setting, $z$ could correspond to parameters of a physical simulator randomized within admissible ranges. 
Samples $\D^\ii$ are drawn from $p(\D)$ by first sampling a random $\latent^\ii$, corresponding to a random system $\sys^\ii$, from $p(\latent)$, together with a random input $\seq{u}^\ii$ from $p(\seq u)$. Then, the output $\seq{y}^\ii$ is obtained by processing input  $\seq{u}^\ii$ through the system $\sys^\ii$. 

In the case of real data, $z$ may correspond to the operating conditions that vary (intentionally or not to the experimenter) across different 
 runs, such as different masses attached to the end-effector of a robot, environment temperature in a chemical reaction, line voltage for electrical equipment, etc.
Furthermore, the exact meaning and (minimum) dimension of $\latent$ might be unknown.


\begin{remark}
The latent variable $\latent$ corresponds to the aspects of the data-generating system that vary across datasets.
%independently on the excitation signal $\seq{u}$.
For instance, in the case of the robot where only the end effector mass changes, a scalar $\latent$ representing such mass suffices, even though the robot itself is characterized by many other physical parameters, that, however, remain fixed for all runs. Those non-varying parameters may be thought of as part of the functional form of the conditional distribution $p(\seq{y} | \seq{u}, z)$ in~\eqref{eq:dataset_distribution}.
\end{remark}

\subsection{Base architecture}
\label{sec:full_order}
Consider the \base\ given by the black-box full-order model structure
\begin{equation}
\label{eq:fullnet}
\hat {\seq{y}} = F(\seq{u}; \fullpar),\;  \fullpar \in \R^{n_\fullpar},
\end{equation}
which we assume is prescribed to describe the input/output dependency $\seq u \rightarrow \seq y$ for each dataset $\D$ in the support of $p(\D)$ with practically no bias, for some system-dependent (unknown) value of the parameter $\theta$. 
We argue that this assumption is not particularly restrictive given the availability of expressive function approximators such as deep neural networks, that, with appropriate selection of hyperparameters, can describe wide classes of nonlinear functions with high accuracy, and have been reported to exhibit competitive performance across different system identification benchmarks~\cite{champneys2024baseline}.

The model structure~\eqref{eq:fullnet} is typically characterized by a number of parameters $n_\fullpar \gg n_\latent$ for two reasons. First, \eqref{eq:fullnet} has a black-box structure, not necessarily tailored to a specific class of system. For example, accurate description of robot dynamics with $l=8$ links may require thousands of black-box parameters ($n_\fullpar \approx 10^3$) with neural-network models~\cite{Bem25}, whereas a physics-based representation only requires $10l = 80$ terms~\cite{gaz2019dynamic}.
Second, just a subset of these physical parameters may be varying across the simulations/experimental runs within the meta-dataset. For instance, as in the example of Remark 1, if the only physical parameter varying across dataset is the robot's end-effector mass, then $n_z=1$ suffices.
Consequently, black-box architectures used to describe physical systems are often heavily over-parameterized.

For a given dataset $D=(\seq{u},\seq{y})$, the optimal parameter that best describes the input/output relation is typically obtained by minimizing a supervised learning criterion:
\begin{equation}
    \label{eq:full_sysid}
    \hat \fullpar = \arg \min_{\fullpar} \loss(\seq y, \hat{\seq y}) + r(\theta), 
\end{equation}
where $\loss(\cdot, \cdot)$ is a training loss that penalizes the mismatch between measurements $\seq y$ and predictions $\hat{\seq y}=F(\seq u, \fullpar)$, and $r:\R^{n_\theta}\to\R$ is a regularization term. A common choice for $\loss(\cdot, \cdot)$
is the mean squared error $
%\hat \theta, \hat x_0 = \arg \min_{\theta, x0} 
\mathrm{MSE}(\seq y, \hat {\seq y}) = \frac{1}{N}\sum_{k=1}^N 
\norm{\seq{y}_k -  \hat {\seq{y}}_k}_2^2$
paired with an $\ell_2$-regularization term $r(\theta)=\rho\|\theta\|_2^2$, where $\rho\geq 0$ controls the regularization strength. 

However, when the number of parameters $n_\theta$ and/or the number of samples $N$ in the training dataset $D$ are large, learning the dynamics represented by $F$ can be computationally intensive. Conversely, when $N$ is small, achieving good {\it generalization} to unseen data becomes difficult.

\subsection{Meta-learning manifolds in the parameter space}
To address the computational and generalization challenges related to full-order system identification, in this paper we address the following problem:
\begin{problem}
Given a full-order architecture~\eqref{eq:fullnet} and a collection $\DD$ of datasets sampled from $p(\D)$, 
learn a lifting function $P: \R^{n_\redpar} \rightarrow \R^{n_\fullpar}$, with $n_\redpar \ll n_\fullpar$ that defines a $n_\redpar$-dimensional manifold $M=\{\theta\in\R^{n_\fullpar}$: $\theta=P(\redpar),\ \redpar \in \R^{n_\redpar}\}$
such that each dataset $D=(\seq{u},\seq{y})$ in the support of $p(D)$ can be described accurately by a model $\hat{\seq{y}}=F(\seq{u};P(\redpar))$ for some coordinate vector $\redpar\in\R^{n_\redpar}$.
\label{prob:manifold}
\end{problem}
We label Problem~\ref{prob:manifold} as a \emph{meta-learning} problem, as our goal is to learn a unique manifold $M$ for the {\it entire meta-dataset}. Once the manifold is learned, the user can fit a model to any dataset $D=(\seq{u},\seq{y})$ contained in the support of $p(\D)$ by solving a reduced-complexity training problem, in that the number of free parameters is (significantly)  reduced from  $n_\fullpar$ to $n_\redpar$.

Problem~\ref{prob:manifold} admits at least one solution for $n_\redpar \geq n_\latent$. Indeed, we 
hypothesized in Section~\ref{sec:full_order} that each data-generating system in the distribution (equivalently, each $\latent \in \R^{n_\latent}$) is represented by (at least) a $\theta \in \R^{n_\theta}$. Then, there exists a function $P^\dagger : \R^{n_\latent} \rightarrow \R^{n_\theta}$ mapping each data-generating system (characterized by a $\latent \in \R^{n_\latent}$) to a parameter $\fullpar \in \R^{n_\fullpar}$ of the full black-box model structure.\footnote{The relationship $\latent \rightarrow \fullpar$ might be one-to-many. The function $P^\dagger$ here arbitrary chooses one of the multiple equivalent solutions.}
Therefore, the model structure 
$\hat {\seq{y}} = F(\seq{u}; P^\dagger(\latent)),\;  \latent \in \R^{n_\latent}$ solves Problem~\ref{prob:manifold} 
with $P=P^\dagger$, $\phi=z$. However, the function $P^\dagger$, as well as $n_z$, are not generally known a priori. 

Let us parameterize function $P$ by a vector $\projpar$ of tunable parameters, $\projpar \in \R^{n_\projpar}$,
and denote it by $P_\gamma$. As we will detail in Section~\ref{sec:reduced_order}, our meta-learning goal is to learn an optimal lifting parameter vector $\hat \projpar \in \R^{n_\projpar}$ for the entire set $\DD$ to define the reduced-complexity architecture
\begin{equation}
\label{eq:rednet}
\hat {\seq{y}} = F(\seq{u}; P_{\hat \projpar}(\redpar)),\;  \redpar \in  \R^{n_\redpar},
\end{equation}
given by the functional composition of the \base\ with  the \emph{learned} lifting function 
$P_{\hat \projpar} : \R^{n_\redpar} \rightarrow \R^{n_\fullpar}$.

Based on the learned reduced-complexity architecture~\eqref{eq:rednet}, and given a new dataset $D^* = (\seq{u}^*, \seq{y}^*)$ that is contained in the support of $p(\D)$, the following reduced-complexity optimization problem
\begin{equation}
    \label{eq:reduced_sysid}
    \hat \redpar = \arg \min_{\redpar} \loss(\seq y^*, F(\seq u^*, P_{\hat\projpar}(\redpar)))
\end{equation}
solves the system identification problem of fitting a model to $D^*$.
Given the reduced number $n_\phi$ of variables, the optimization problem~\eqref{eq:reduced_sysid} can be solved much more efficiently than~\eqref{eq:full_sysid}, e.g., by taking advantage of quasi-Newton approaches (see, e.g.,~\cite{Bem25,Bem23}) that might not be computationally feasible in the full-order structure~\eqref{eq:fullnet}. Furthermore, the risk of overfitting is significantly mitigated by the reduced number of free parameters.

\begin{remark}
%We remark that, i
In case $p(\D)$ is a synthetic dataset distribution, a parametric architecture with parameters $z$ describing the dependency $\seq u \rightarrow \seq y$ is already available a priori, and it corresponds to the conditional density $p(\seq y | \seq u, z)$ in \eqref{eq:dataset_distribution}. Nonetheless, such architecture may be defined in terms of complex mathematical objects such as stiff ordinary differential equations, partial differential equations, implicit algebraic constraints, non-differentiable blocks, or components subject to restricted interaction in the case of commercial simulators. Conversely, the learned architecture \eqref{eq:rednet} allows for full customization by the user.
\end{remark}

\section{Meta-learning procedure}
\label{sec:reduced_order}
In line with the standard meta-learning setting, we split each dataset 
$D^\ii$ into training and test portions, i.e.,
%consider a slightly modified stream 
%of datasets
\begin{equation}
\label{eq:dataset_stream_split}
D^\ii = (\seq{u}^\ii_\tr , \seq{y}^\ii_\tr ), (\seq{u}^\ii_\te , \seq{y}^\ii_\te ),\, i=1,2,\dots,
\end{equation}
where both $\seq{y}^\ii_\tr$ and $\seq{y}^\ii_\te$ are generated by the \emph{same} system $S^\ii$ for different input signals $\seq{u}^\ii_\tr$ and $\seq{u}^\ii_\te$, respectively.
Such a data split 
allows one to learn an optimal manifold $M$ such that, when training is performed on $(\seq{u}^\ii_\tr , \seq{y}^\ii_\tr )$, performance measured on test data 
$(\seq{u}^\ii_\te , \seq{y}^\ii_\te )$ is good. %In other words, 


A direct way of learning an optimal parametric function $P_{\hat \projpar}$ is to solve
the following bilevel optimization problem:
\begin{subequations}
\label{eq:bilevel}
\begin{align}
\hat \projpar &= \arg \min_{\projpar} \E_{p(\D)}
\big [ \loss({\seq y}_\te, \hat {\seq y}_\te) \big ]\\
\hat{\seq{y}}_\te  &= F\left(
  \seq{u}_\te ; \,
  P_{\projpar} ( \hat \redpar  )
\right)\\
  \hat \redpar &=
  \text{SYSID}(\seq u_\tr , \seq y_\tr ),\label{eq:sysid}
\end{align}
\end{subequations}
where SYSID is the system identification algorithm that the modeler intends to use later on the real data
to fit $\phi$ on a given dataset $D$.  The most obvious choice for SYSID is to align the inner-loop optimization 
with the intended reduced-complexity fitting criterion~\eqref{eq:reduced_sysid}:
%For instance, empirical risk minimization on the training dataset corresponds to:
\begin{equation}
\label{eq:inner_optimization}
\mathrm{SYSID} (\seq u_\tr , \seq y_\tr ) = 
\arg \min_\phi \loss(F(\seq u_\tr ; \, P_\projpar(\redpar)), \seq y_\tr ).
\end{equation}
In essence, \eqref{eq:bilevel} explicitly tunes the lifting function $P_{\hat \gamma}$ to provide optimal expected performance, when a specific system identification algorithm is applied in training.

\subsection{Encoder-decoder architecture}
\label{enc_dec_hypernet}
To circumvent the numerical intricacies related to the nested optimization problem formulation~\eqref{eq:bilevel}, we replace the inner optimization with an auxiliary neural network $E_{\psi}$, which may be interpreted as a \emph{learned} black-box SYSID algorithm:
\begin{subequations}
\label{eq:hypernet}
\begin{align}
\hat \projpar, \hat \encpar &= \arg \min_{\projpar, \encpar} \E_{p(\D)} 
\overbrace{\big [ \loss({\seq y}_\te, \hat {\seq y}_\te,) \big ]}^{J(\gamma, \psi)} \label{eq:hypernet_a}\\
\hat{\seq{y}}_\te  &= F\left(
  \seq{u}_\te ; \,
  P_{\projpar} ( \hat \redpar )
\right) \label{eq:decoder}\\
  \hat \redpar &=
  E_{\psi}(\seq u_\tr , \seq y_\tr ) \label{eq:encoder}.
\end{align}
\end{subequations}
Note that $E_{\psi}$ is an (approximate) multiparametric solution~\cite{Fia83} to the optimization problem associated with~\eqref{eq:sysid}, whose unknowns are the entries of $\redpar$ and parameters are $(\seq u_\tr , \seq y_\tr )$.

The pair \eqref{eq:decoder}-\eqref{eq:encoder} may also be seen as an encoder-decoder architecture. The encoder $E_\encpar$ ingests the input-output pair $(\seq u_\tr , \seq y_\tr )$ and outputs reduced-complexity parameters $\hat \redpar$. The decoder, defined by the functional composition $F(\cdot, P_{\hat \projpar}(\cdot))$, first lifts the reduced parameters  $\hat \phi$ to the original full-order dimension $n_\theta$, and then applies the \base\ with the lifted parameters to the input $\seq u_\te $ to predict the output $\hat {\seq y}_\te $. Figure~\ref{fig:encdec_hypernet} is a graphical representation of~\eqref{eq:decoder}-\eqref{eq:encoder}
focusing on this encoder-decoder interpretation. Finally, we highlight that the encoder plays the role of a hyper-network~\cite{ha2016hypernetworks}, as it provides the parameters of another neural network, namely of the \base\ $F$.

To solve~\eqref{eq:hypernet}, we replace the intractable expectation over $p(\D)$
by a Monte Carlo average. In other words, $J(\gamma, \psi)$ in~\eqref{eq:hypernet} is approximated as a mean  over $b$ sampled datasets $D^\ii$ from the meta-dataset $\DD$ leading to:
\begin{equation}
    \label{eq:monte_carlo}
    \tilde J(\gamma, \psi) = \frac{1}{b} \sum_{i=1}^b \loss\bigg(\seq y_\te^\ii, F\big(\seq u_\te^\ii, P_\projpar(E_\psi(\seq u_\tr^\ii, \seq y_\tr^\ii)\big ) \bigg)
\end{equation}
The loss $ \tilde J(\gamma, \psi)$ is minimized with standard gradient-based optimization. In case the datasets $D^\ii$ are resampled at each iteration, $b$ plays the role of the batch size.

%\begin{remark}
We emphasize that the main objective of solving \eqref{eq:hypernet} is to find the lifting function $P_{\hat \projpar}$ to be plugged into~\eqref{eq:rednet}, and, therefore, the manifold $M$ of dimension $n_\redpar$ that best reduces the complexity of identifying a model over the dataset distribution $p(\D)$.
Once this mapping $P_{\hat \projpar}$ is learned, given a new dataset $D^* = (\seq{u}^*, \seq{y}^*)$, the user may prefer to solve the identification problem~\eqref{eq:reduced_sysid} explicitly as in~\eqref{eq:sysid}, instead of relying on the learned algorithm $E_{\hat \encpar}$. In fact, solving~\eqref{eq:reduced_sysid} is arguably more trustworthy than relying on a black-box map. Nonetheless, the learned algorithm $E_{\hat \encpar}$ may be used to initialize an iterative algorithm to evaluate SYSID in~\eqref{eq:sysid}, thereby speeding up the reduced-complexity model learning task~\eqref{eq:sysid}.% and possibly escaping local minima.
%\end{remark}


\begin{figure}
\begin{center}
\includegraphics[width=.6\hsize]{img/ae_archi.pdf}
\caption{The autoencoder architecture for reduced-complexity architecture learning.}
\label{fig:encdec_hypernet}
\end{center}
\end{figure}

\subsection{Specialization for neural state-space models}
We now specialize the methodology presented in the previous sections for datasets $\D^\ii$ that are input/output time series generated by dynamical systems, and the \base\ is a state-space model:

\begin{equation}
\label{eq:generic_ss}
%\begin{align}
    x_{k+1} = f(x_k, u_k; \theta) \qquad
    y_{k} = g(x_k, u_k; \theta).
%\end{align}
\end{equation}
In~\eqref{eq:generic_ss}, $f$ and $g$ are learnable mappings, such as feed-forward neural networks.
In this case, the output sequence $\seq y$, given an input sequence $\seq u$, also depends on an initial state $x_0$. Formally, we have: $\hat {\seq y} = F(\seq u, x_0; \theta)$.
Thus, in applying the previously mentioned methodology, care has to be taken in the initialization of this state, notably for a correct computation of the loss in~\eqref{eq:hypernet_a}.

A few practical solutions have been introduced in recent SYSID literature to tackle the initial state issue, see~\cite{forgione2022learning} for an overview. Similarly to \cite{masti2021learning,beintema2023deep,Bem25}, an additional neural network can be included to processes the first $n_{\rm init}$ samples of $(\seq u_\te , \seq y_\te )$ and provide an estimate $\hat x_{n_{\rm init}}$ of the state at time step $n_{\rm init}$. Starting from this state estimate, the loss~\eqref{eq:hypernet_a} may be computed, excluding the first $n_{\rm init}$ samples from the loss evaluation. We note that, in the case of synthetic data, the two datasets $(\seq u_\tr , \seq y_\tr )$, $(\seq u_\te , \seq y_\te )$ may be chosen to have the \emph{same} (random) initial state. In this case, the encoder network $E_\psi$ may be designed to provide initial state estimates, together with the reduced-complexity model parameters, thereby simplifying the architecture. 

Finally, as argued in~\cite{forgione2022learning}, an even simpler approach may be followed when the systems to be modeled are known to have a fading memory. In this case, the state can be initialized to an arbitrary value (such as zero), and the effect of the system's ``natural response'' can be removed from the loss by ignoring the contribution from the first $n_{\rm skip}$ samples, where $n_{\rm skip}$ is selected large enough. In the numerical examples presented in Section~\ref{sec:examples}, for simplicity, problem~\eqref{eq:hypernet} is instantiated relying on such a ``zero'' state initialization scheme. More advanced schemes may be integrated with a moderate effort.

\section{Numerical illustration}
\label{sec:examples}
We apply our approach to the Bouc-Wen system identification benchmark~\cite{noel2016hysteretic}. 
The software was developed in Python using the JAX automatic differentiation library~\cite{JAX} and it is available at the repository \url{https://github.com/forgi86/sysid-neural-manifold}. All computations were performed on a server equipped with an AMD EPYC 9754 CPU and an Nvidia 4090 GPU.

\subsection{The Bouc-Wen benchmark}
The Bouc-Wen benchmark entails the vibrations of a 1-DoF hysteretic mechanical system. The system dynamics are described in state-space form by:

\begin{subequations}  
\label{eq:boucwen}
\begin{align}
%\overbrace{
\begin{bmatrix} 
\dot{p}(t) \\ 
\dot{v}(t) \\ 
\dot{z}(t) 
\end{bmatrix}
%}^{x(t)}
&=
\begin{bmatrix} 
v(t) \\ 
\frac{1}{m_L} \left (u(t) -k_L p(t) - c_L v(t) - z(t)  \right) \\ 
\alpha v(t) - \beta(\gamma |v(t)| |z(t)|^{\nu -1} + \delta v(t) |z(t)|^\nu)
\end{bmatrix}\\
y(t) &= p(t),
\end{align}
\end{subequations}
%\end{small}
where $p(t)$~(m) is the measured output position; $v(t)$~(m/s) is the unmeasured velocity; $z(t)$~(N) is an unmeasured hysteretic force term; and $u(t)$~(N) is the known input force. The nominal value of the model coefficients are reported in the first row of Table~\ref{tab:bw_params}.
\begin{table}[!ht]
\setlength{\tabcolsep}{4pt}
\centering
\caption{Coefficients of the Bouc-Wen system. Nominal values (first row) and minimum/maximum values in the dataset distribution (second/third row).}
\label{tab:bw_params}
\small
\begin{tabular}{c||cccccccc}
\toprule
 & $m_L$ & $c_L$ & $k_L$ & $\alpha$ & $\beta$ & $\gamma$ & $\delta$ &$\nu$\\
\midrule
nom & 2 & 10 &$5.0\times 10^4$ & $5.0\times 10^4$ & 1000 & 0.8 & -1.1 & 1\\
min & 1 & 5 & $2.5\times 10^4$ & $2.5\times 10^4$ & $500$ & 0.5 & -1.5 &1\\
max & 3 & 15 & $7.5\times 10^4$ & $7.5\times 10^4$ & 4500 & 0.9 & -0.5 &1\\
\bottomrule
\end{tabular} 
\end{table}

The benchmark provides a test dataset where the input $u(t)$ is a random-phase multisine of length $8192$ sampled at frequency $f_s=750$~Hz  that excites the frequency range $[5, 150]$~Hz and has a root mean square of $50$~N. System~\eqref{eq:boucwen} is simulated with 
its nominal coefficients and the test output $y(t) = p(t)$ is provided noise-free.

A ``default'' training dataset is also provided,  comprising 40960 samples generated by exciting the system with a multisine input signal with the same characteristic as the test input. In the training dataset, the output is corrupted by an additive Gaussian noise with bandwidth $[0 -375]$~Hz and amplitude $8 \cdot 10^{-3}$. Moreover, the benchmark provides simulation code and specifies that the user is free to choose different input signals to excite the system and generate arbitrary training datasets. 

In this paper, along the lines of \cite{chakrabarty2025meta}, we extend the original scope of the benchmark~\cite{noel2016hysteretic} and use the simulator to generate datasets with perturbed coefficient values, aiming at finding a reduced-complexity architecture that provides high performance over the entire \emph{class} of Bouc-Wen systems with parameters comprised in the min-max range reported in Table~\ref{tab:bw_params}. 


\subsection{Base architecture}
As \base,\ we consider the neural state-space structure:
\begin{subequations}
\label{eq:ss_boucwen}
\begin{align}
    x_{k+1} &= A x_k + B u_x + N_f(x_k, u_k; W_f) \\
    y_{k} &= C x_k +  N_g(x_k; W_g),
\end{align}
\end{subequations}
where $n_u=1$, $n_y=1$, $n_x=3$, $N_f$, and $N_g$ are feed-forward neural networks with one hidden layer and $16$ hidden units with weights $W_f$ and $W_g$, respectively, and \texttt{tanh} activation function; and $A$, $B$, and $C$ are matrices of proper size. The \base\ parameters are thus $\theta = \mathrm{vec}(W_f,\ W_g,\ A,\ B,\ C) \in \R^{n_\fullpar}$, with $n_\fullpar=244$. 
Note that a very similar architecture has been utilized in~\cite{schoukens2021improved} and shown to provide state-of-the-art results on this nonlinear system identification benchmark.

\subsection{Metrics and baselines}
We evaluate model performance in terms of the indices
\begin{equation*}
\begin{aligned}
\mathrm{fit} = 100\cdot \left(1- \frac{\sqrt{\sum_{k=0}^{\nsamp-1} \left(\seq{y}_k -  \hat {\seq{y}}_k\right)^2} }  
{\sqrt{\sum_{k=0}^{\nsamp-1} \left(\seq{y}_k -  {\overline{\seq{y}}}\right)^2}}\right) (\%), \qquad
\mathrm{rmse} = \sqrt{\frac{1}{\nsamp} \sum_{k=0}^{\nsamp-1} \left(\seq{y}_k -  \hat {\seq{y}}_k\right)^2},
\end{aligned}
\end{equation*}
where ${\overline{\seq{y}}}$ is the sample average of the measured output $\seq y$.

Fitting the model on the default $40960$-length training dataset, \cite{schoukens2021improved} reported the state-of-the-art result on this benchmark ($\mathrm{fit}=98.9~\%$, $\mathrm{rmse}=7.16\cdot 10^{-6}$) on the test set. 
We were able to closely match this result ($\mathrm{fit}=98.8~\%$, $\mathrm{rmse}=7.87\cdot 10^{-6}$) by minimizing the MSE loss
of the \base\ over 40000 iterations of \texttt{AdamW} with learning rate $10^{-3}$ followed by 10000 iterations of \texttt{BFGS}. The training time was~1.4 hours.
%The small deviation may be due to slightly different weight initialization, optimization procedure, minor architectural differences, and random fluctuations.
The eigenvalues of the Hessian of the full-order training loss at the solution of the optimization problem
%~\eqref{eq:full_sysid} 
are visualized in Figure~\ref{fig:boucwen_hessian}. 
Interestingly, 
many of those eigenvalues are close to zero, which suggests that the model structure is over-parameterized and further motivates the pursue for a lower-dimensional parameterization.
\begin{figure}
    \centering
    \includegraphics[width=.55\hsize]{img/hessian_eigenvalues.pdf}
    \caption{Eigenvalues of the Hessian of the full-order training loss.}
    \label{fig:boucwen_hessian}
\end{figure}

We also train as a baseline a third-order linear model that provides $\mathrm{fit}=77.2~\%$, underlining the relevance of non-linearities in this benchmark.


\subsection{Full-order model training}
\label{sec:full_order_training}
To assess the performance of full-order model learning in the low-data regime, we train models over shorter sequences of length 
%$L=$ $100$, $200$, $300$, $400$, $500$, $600$, $700$, $800$, $900$, $1000$, $2000$, $3000$, $4000$, $5000$ samples.
$L=$ $100$, $200$, $400$, $500$, $600$, $800$, $1000$, $2000$, $3000$, $4000$, $5000$ samples. 
For each length $L$, we execute a Monte Carlo study and train 100 distinct models over 100 $L$-length subsequences randomly extracted from the official $40960$-length dataset.  
For optimization, we use the same \texttt{AdamW} + \texttt{BFGS} approach with the same settings detailed in the previous section.
As shown in Figure~\ref{fig:boucwen_boxplot}, full-order model learning proves highly inefficient in the low-data regime. In particular, for sequence length $L \leq 400$, the median $\mathrm{fit}$ is below the linear baseline. 
%Furthermore, a significant number of runs whose performance is below the linear baseline are observed for $L \leq 800$ samples.
The median performance consistently improves for increasing sequence length, with average \textrm{fit} above $97~\%$ for $L\geq 3000$ samples. 
However, even for $L=2000, 3000, 4000$, and $5000$, a small number of runs (2, 1, 1, and 1, respectively) fail due to numerical issues in the \texttt{BFGS} solver. While these issues might be mitigated with a careful tuning of the solver settings, they highlight the challenges of full-order model training.

Figure~\ref{fig:training_time} illustrates the time required to train in parallel the 100 models vs. the sequence length $L$. We report both the run time for the $40000$ \texttt{AdamW} iterations and the total training time, which includes the $10000$ \texttt{BFGS} iterations. The results show that \texttt{BFGS} dominates the overall training time.

\begin{figure}
    \centering
    \includegraphics[width=.8\hsize]{img/boxplot_paper.pdf}
    \caption{Full and reduced-order models: test FIT vs. training sequence length $L$.}
    \label{fig:boucwen_boxplot}
\end{figure}

\begin{figure}
\centering
    \includegraphics[width=.8\hsize]{img/training_times_paper.pdf}
    \caption{Training time vs. training sequence length $L$.}
    \label{fig:training_time}
\end{figure}


\subsection{Meta-learning the reduced-complexity architecture}
\label{sec:meta_training}
To enable higher performance in the low-data regime, we learn a manifold with $n_\phi=20$ parameters, embedded within the full-order architecture~\eqref{eq:ss_boucwen} characterized by~$n_\theta=244$ parameters, by applying the methodology described in Section~\ref{sec:reduced_order}.
To this aim, we introduce the parameterized lifting network $P_\projpar : \R^{n_\phi} \rightarrow \R^{n_\theta} = V \phi + \theta_{\rm bias}$ with $V \in \R^{n_\theta \times n_\phi}, \theta_{\rm bias} \in \R^{n_\theta}$,  which expands the reduced-complexity dimension $n_\phi$ to the full-order $n_\theta$. Its tunable parameters are collected in the vector $\projpar = \mathrm{vec}(V, \theta_{\rm bias}) \in \R^{n_\projpar}$, with $n_\projpar = (n_\redpar + 1) n_\fullpar = 5124$. 
%Note that the term $\theta_{\rm bias}$ may be interpreted as a ``center'' point, such as the initialization learned by MAML.% \AB{MAML undefined so far}.

 To learn the optimal lifting network parameters $\hat \projpar$, we adopt the encoder-decoder methodology and solve the optimization 
problem~\eqref{eq:hypernet}, using the Monte Carlo approximation $\tilde J(\gamma, \psi)$ in~\eqref{eq:monte_carlo} 
of the loss $J(\projpar, \encpar)$.
The meta-dataset $\DD$ is obtained by randomly sampling Bouc-Wen systems with coefficients uniformly distributed within the min-max ranges in Table~\ref{tab:bw_params}, and feeding as input random multisine sequences of length $N=2000$ with the same properties as the default training input.

The encoder $E_\psi$ is a sequence-to-vector architecture obtained by combining a bi-directional Gated Recurrent Unit (GRU) with a feed-forward neural network. The GRU is configured with $n_u+n_y=2$ input channels and a single hidden layer with $n_h=128$ hidden units.
It processes the sequences $(\seq u_\tr , \seq y_\tr )$ and produces a sequence of activations  $\seq h_{0:N-1}$, $\seq{h}_i \in \R^{n_h}$. This sequence is reduced to a single vector $h_m$ through \emph{average pooling}: $h_m = \frac{1}{N} \sum_i{\seq {h}_i}$ and further processed by the feed-forward neural network configured with 
$n_h=128$ inputs, a hidden layer with 128 units, and an output layer with 
$n_\phi = 20$ units, and \texttt{tanh} activation functions. Overall, the network $E_\encpar$ has $n_\encpar=136340$ parameters corresponding to the GRU and the feed-forward neural network weights. It takes as input the sequences $(\seq u_\tr , \seq y_\tr )$ and produces as output a single vector of size $n_\phi$.

For gradient-based minimization of the loss~\eqref{eq:monte_carlo}, the \texttt{Adam} algorithm is applied with a batch size of 128 and initial learning rate of $2 \cdot 10^{-4}$, decreased to $2 \cdot 10^{-5}$ with cosine scheduling over $200000$ iterations. The optimization takes approximately 25.3 hours. As a result, we obtain the reduced-order architecture
\eqref{eq:rednet} and, as a byproduct, the encoder network $E_{\hat \encpar}$ which can be used to initialize the reduced-complexity model learning~\eqref{eq:reduced_sysid}.

\subsection{Reduced-complexity model training}
\label{sec:reduced_order_training}
We repeat the Monte Carlo study of Section~\ref{sec:full_order_training} using the reduced-order architecture meta-learned in Section~\ref{sec:meta_training}, instead of the \base. In all the runs, the loss~\eqref{eq:reduced_sysid} is minimized over $10000$ iterations of \texttt{AdamW} with learning rate $10^{-3}$, using the encoder network $E_{\hat \psi}$ to obtain an initial guess of $\phi$. Results in Figure~\ref{fig:boucwen_boxplot} show that the reduced-complexity training is significantly more effective than the full-order one for sequences of length up to 2000 samples. Very good results are obtained with just 500 samples (median fit of $95.2\%$, median $\mathrm{rmse} = 3.18\cdot 10^{-5}$). For training sequences longer than 2000 samples, full-order training generally yields superior results, given the higher flexibility of the model structure. 

We remark that none of the reduced-order training runs exhibited numerical difficulties, and the obtained performance is largely insensitive to the choice of the optimizer and its settings. This underscores the stability and reliability of the reduced-order model learning problem.

The run time of reduced-order training is visualized in Figure~\ref{fig:training_time} and corresponds to approximately $25~\%$ the \texttt{AdamW} portion of the full-order case. This is because the cost of a single \texttt{AdamW} iteration remains nearly identical for both cases, as evaluating the lifting function $P$ is negligible compared to computing the training loss. Due to the lower problem dimensionality and to the effective optimization initialization
using $E_{\hat \phi}$, $10000$ \texttt{AdamW} iterations are sufficient for reduced-order model training.


To analyze the sensitivity of the methodology to the choice of the manifold dimensionality, we repeat the reduced-complexity architecture meta-learning and the Monte Carlo study for $n_\redpar=10$. Results in Figure~\ref{fig:boucwen_boxplot} highlight that this choice leads to the best results for the very short sequences of length $L=100$, while it leads to worse performance against $n_\redpar=20$ for all other lengths.

\section{Conclusions}
\label{sec:conclusions}
This paper introduced a meta-learning framework for system identification that discovers a low-dimensional manifold within the parameter space of an over-parameterized neural architecture. Our results on the Bouc-Wen benchmark highlight the potential of this approach, particularly in extremely data-scarce scenarios.

Several extensions of this framework can be explored. While our methodology was introduced in the context of a black-box base architecture, it could be extended to models, constraints, and fitting criteria informed by physical knowledge. Additionally, integrating variational learning techniques into the meta-learning process could enable the discovery of not only a deterministic low-dimensional representation but also a prior distribution over this space. This would support a principled probabilistic approach to reduced-complexity modeling, where uncertainty is propagated from a meta-learned prior, enhancing robustness, generalization, and interpretability.

\bibliographystyle{abbrv}
\bibliography{biblio}

\end{document}
